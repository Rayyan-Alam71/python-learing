{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd9c3b4-9954-490a-bb0e-eb1312823654",
   "metadata": {},
   "source": [
    "# TOKENIZATION , LEMMATIZATION , SERIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b2cd9-9d15-487c-b884-5a91d20dfd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use nltk library for building python programs to work with human language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11da58ed-ce11-4a8e-85dc-c3341245f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\aio\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\aio\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\aio\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aio\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aio\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\aio\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d5aaa15-3849-410a-9efe-5515258564a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to the world of GenAI. This is gonna be the future.\n",
      "2025 is going to be the future of AI.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining your own corpus\n",
    "\n",
    "corpus = \"\"\"Hello welcome to the world of GenAI. This is gonna be the future.\n",
    "2025 is going to be the future of AI.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89846b3a-aa77-4a6a-83c4-92d5348a3b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to the world of GenAI. This is gonna be the future.\n",
      "2025 is going to be the future of AI.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b063f77-ea14-4c25-939f-2b5912d4ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\AIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bcbb3f6-0030-42b6-9db1-6db138153556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIO\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt_tab\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.find(\"tokenizers/punkt_tab\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65862e7e-e5e0-49d8-9671-005a22bd2387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is Rayyan Alam.', 'And I am 20 years old.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"My name is Rayyan Alam. And I am 20 years old.\"\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b7b0b1a-aeba-4088-91cc-f28314a9ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus  = \"\"\"Hello! my name is Rayyan Alam. I a getting my hands dirty on GenAI. 2025 is going to be\n",
    "    the AI's year.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f93bd50-14d7-45af-98c9-b6b1e39e57f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! my name is Rayyan Alam. I a getting my hands dirty on GenAI. 2025 is going to be\n",
      "    the AI's year.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "documents = sent_tokenize(corpus)\n",
    "\n",
    "# Notedown : it is toekinizing on the basis of exclamation marks too, along with the \n",
    "#           full stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f4c659-5299-4094-a788-920267598918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "my name is Rayyan Alam.\n",
      "I a getting my hands dirty on GenAI.\n",
      "2025 is going to be\n",
      "    the AI's year.\n"
     ]
    }
   ],
   "source": [
    "type(documents)\n",
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9384acd2-4cad-41cf-bdba-bf67338193df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "\n",
    "## Paragraphs -> words\n",
    "## sentences -> words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb729db-9655-45c9-b878-2a2af30f62be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Hello\n",
      "!\n",
      "my\n",
      "name\n",
      "is\n",
      "Rayyan\n",
      "Alam\n",
      ".\n",
      "I\n",
      "a\n",
      "getting\n",
      "my\n",
      "hands\n",
      "dirty\n",
      "on\n",
      "GenAI\n",
      ".\n",
      "2025\n",
      "is\n",
      "going\n",
      "to\n",
      "be\n",
      "the\n",
      "AI\n",
      "'s\n",
      "year\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(type(words))\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aa36950-57b8-4c18-b2e1-59cfec1fa224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For line 1\n",
      "['Hello', '!']\n",
      "\n",
      "\n",
      "For line 2\n",
      "['my', 'name', 'is', 'Rayyan', 'Alam', '.']\n",
      "\n",
      "\n",
      "For line 3\n",
      "['I', 'a', 'getting', 'my', 'hands', 'dirty', 'on', 'GenAI', '.']\n",
      "\n",
      "\n",
      "For line 4\n",
      "['2025', 'is', 'going', 'to', 'be', 'the', 'AI', \"'s\", 'year', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentence -> words\n",
    "from nltk.tokenize import word_tokenize\n",
    "line =1\n",
    "for sentence in documents:\n",
    "    print(\"For line\",line)\n",
    "    print(word_tokenize(sentence))\n",
    "    line +=1\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18d5cb84-550e-4844-ab4d-79bf1465f02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Rayyan',\n",
       " 'Alam',\n",
       " '.',\n",
       " 'I',\n",
       " 'a',\n",
       " 'getting',\n",
       " 'my',\n",
       " 'hands',\n",
       " 'dirty',\n",
       " 'on',\n",
       " 'GenAI',\n",
       " '.',\n",
       " '2025',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'AI',\n",
       " \"'\",\n",
       " 's',\n",
       " 'year',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)\n",
    "\n",
    "# Note: There is  difference between word_tokenize and wordpuct_tokenize.  word_tokenize \n",
    "# normally tokenize (Alam's => [\"Alam\", \" 's \"]),  while\n",
    "# wordpunct_tokenize (Alam's => [\"Alam\", \" ' \", \"s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21134ddc-703c-495b-bb4c-6d4b4f6f2456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'my', 'name', 'is', 'Rayyan', 'Alam', '.', 'I', 'a', 'getting', 'my', 'hands', 'dirty', 'on', 'GenAI', '.', '2025', 'is', 'going', 'to', 'be', 'the', 'AI', \"'s\", 'year', '.']\n",
      "['Hello', '!', 'my', 'name', 'is', 'Rayyan', 'Alam', '.', 'I', 'a', 'getting', 'my', 'hands', 'dirty', 'on', 'GenAI', '.', '2025', 'is', 'going', 'to', 'be', 'the', 'AI', \"'\", 's', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "words_punct = wordpunct_tokenize(corpus)\n",
    "\n",
    "# comparison\n",
    "\n",
    "print(words)\n",
    "print(words_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8974694a-bb45-436f-9fd8-d61efb0466ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'my', 'name', 'is', 'Rayyan', 'Alam.', 'I', 'a', 'getting', 'my', 'hands', 'dirty', 'on', 'GenAI.', '2025', 'is', 'going', 'to', 'be', 'the', 'AI', \"'s\", 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "# next one is TreebankWordTokenizer\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "words_treebank = tokenizer.tokenize(corpus)\n",
    "print(words_treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83190d65-bb62-4ca6-8458-467894da6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : In the treebankWordTokenizer, it considers only last full-stop as an alone word.\n",
    "# otherwise for the full-stops occuring in the mid-way, it does not separate (Eg. => GenAI. AND ' . ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
