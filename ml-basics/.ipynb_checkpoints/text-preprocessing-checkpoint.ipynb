{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd9c3b4-9954-490a-bb0e-eb1312823654",
   "metadata": {},
   "source": [
    "# TOKENIZATION , LEMMATIZATION , SERIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b2cd9-9d15-487c-b884-5a91d20dfd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use nltk library for building python programs to work with human language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11da58ed-ce11-4a8e-85dc-c3341245f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\aio\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\aio\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\aio\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aio\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aio\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\aio\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d5aaa15-3849-410a-9efe-5515258564a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to the world of GenAI. This is gonna be the future.\n",
      "2025 is going to be the future of AI.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining your own corpus\n",
    "\n",
    "corpus = \"\"\"Hello welcome to the world of GenAI. This is gonna be the future.\n",
    "2025 is going to be the future of AI.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89846b3a-aa77-4a6a-83c4-92d5348a3b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to the world of GenAI. This is gonna be the future.\n",
      "2025 is going to be the future of AI.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b063f77-ea14-4c25-939f-2b5912d4ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\AIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bcbb3f6-0030-42b6-9db1-6db138153556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIO\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt_tab\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.find(\"tokenizers/punkt_tab\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65862e7e-e5e0-49d8-9671-005a22bd2387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is Rayyan Alam.', 'And I am 20 years old.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"My name is Rayyan Alam. And I am 20 years old.\"\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b7b0b1a-aeba-4088-91cc-f28314a9ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus  = \"\"\"Hello! my name is Rayyan Alam. I a getting my hands dirty on GenAI. 2025 is going to be\n",
    "    the AI's year.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f93bd50-14d7-45af-98c9-b6b1e39e57f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! my name is Rayyan Alam. I a getting my hands dirty on GenAI. 2025 is going to be\n",
      "    the AI's year.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "documents = sent_tokenize(corpus)\n",
    "\n",
    "# Notedown : it is toekinizing on the basis of exclamation marks too, along with the \n",
    "#           full stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f4c659-5299-4094-a788-920267598918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "my name is Rayyan Alam.\n",
      "I a getting my hands dirty on GenAI.\n",
      "2025 is going to be\n",
      "    the AI's year.\n"
     ]
    }
   ],
   "source": [
    "type(documents)\n",
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9384acd2-4cad-41cf-bdba-bf67338193df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "\n",
    "## Paragraphs -> words\n",
    "## sentences -> words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb729db-9655-45c9-b878-2a2af30f62be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Hello\n",
      "!\n",
      "my\n",
      "name\n",
      "is\n",
      "Rayyan\n",
      "Alam\n",
      ".\n",
      "I\n",
      "a\n",
      "getting\n",
      "my\n",
      "hands\n",
      "dirty\n",
      "on\n",
      "GenAI\n",
      ".\n",
      "2025\n",
      "is\n",
      "going\n",
      "to\n",
      "be\n",
      "the\n",
      "AI\n",
      "'s\n",
      "year\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(type(words))\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aa36950-57b8-4c18-b2e1-59cfec1fa224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For line 1\n",
      "['Hello', '!']\n",
      "\n",
      "\n",
      "For line 2\n",
      "['my', 'name', 'is', 'Rayyan', 'Alam', '.']\n",
      "\n",
      "\n",
      "For line 3\n",
      "['I', 'a', 'getting', 'my', 'hands', 'dirty', 'on', 'GenAI', '.']\n",
      "\n",
      "\n",
      "For line 4\n",
      "['2025', 'is', 'going', 'to', 'be', 'the', 'AI', \"'s\", 'year', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentence -> words\n",
    "from nltk.tokenize import word_tokenize\n",
    "line =1\n",
    "for sentence in documents:\n",
    "    print(\"For line\",line)\n",
    "    print(word_tokenize(sentence))\n",
    "    line +=1\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18d5cb84-550e-4844-ab4d-79bf1465f02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Rayyan',\n",
       " 'Alam',\n",
       " '.',\n",
       " 'I',\n",
       " 'a',\n",
       " 'getting',\n",
       " 'my',\n",
       " 'hands',\n",
       " 'dirty',\n",
       " 'on',\n",
       " 'GenAI',\n",
       " '.',\n",
       " '2025',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'AI',\n",
       " \"'\",\n",
       " 's',\n",
       " 'year',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)\n",
    "\n",
    "# Note: There is  difference between word_tokenize and wordpuct_tokenize.  word_tokenize \n",
    "# normally tokenize (Alam's => [\"Alam\", \" 's \"]),  while\n",
    "# wordpunct_tokenize (Alam's => [\"Alam\", \" ' \", \"s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21134ddc-703c-495b-bb4c-6d4b4f6f2456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'my', 'name', 'is', 'Rayyan', 'Alam', '.', 'I', 'a', 'getting', 'my', 'hands', 'dirty', 'on', 'GenAI', '.', '2025', 'is', 'going', 'to', 'be', 'the', 'AI', \"'s\", 'year', '.']\n",
      "['Hello', '!', 'my', 'name', 'is', 'Rayyan', 'Alam', '.', 'I', 'a', 'getting', 'my', 'hands', 'dirty', 'on', 'GenAI', '.', '2025', 'is', 'going', 'to', 'be', 'the', 'AI', \"'\", 's', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "words_punct = wordpunct_tokenize(corpus)\n",
    "\n",
    "# comparison\n",
    "\n",
    "print(words)\n",
    "print(words_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8974694a-bb45-436f-9fd8-d61efb0466ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'my', 'name', 'is', 'Rayyan', 'Alam.', 'I', 'a', 'getting', 'my', 'hands', 'dirty', 'on', 'GenAI.', '2025', 'is', 'going', 'to', 'be', 'the', 'AI', \"'s\", 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "# next one is TreebankWordTokenizer\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "words_treebank = tokenizer.tokenize(corpus)\n",
    "print(words_treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83190d65-bb62-4ca6-8458-467894da6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : In the treebankWordTokenizer, it considers only last full-stop as an alone word.\n",
    "# otherwise for the full-stops occuring in the mid-way, it does not separate (Eg. => GenAI. AND ' . ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb361b1-a7cd-45f2-88b7-6a857fd44001",
   "metadata": {},
   "source": [
    "## Text-Preprocessing => STEMMING USING NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e9957-829b-4d44-becd-a2227efd4628",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its word stem that affixxes to suffixes \n",
    "and prefixes or to the roots of words known as a lemma. Stemming is important in NLU \n",
    "(Natural Language Understanding) and NLP.\n",
    "\n",
    "It is used in solving problems like Classification Problems;\n",
    "    Like Comments of product is a positive or a negative review\n",
    "     where Reviews ---> words like eating , eat, eats, eaten are considered to be +ve.\n",
    "Thus here \"eat\" is a word-stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958e836f-6b0c-4f80-b4b6-5b1968b2ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"written\", \"programming\", \"programs\", \"programmed\", \"history\", \"finally\", \"finalize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ab69d-2f1a-47a6-a9bd-056262a443c8",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18221bbf-c11c-40e8-873b-7cf8b3c54f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eaten\n",
      "writing ---> write\n",
      "writes ---> write\n",
      "written ---> written\n",
      "programming ---> program\n",
      "programs ---> program\n",
      "programmed ---> program\n",
      "history ---> histori\n",
      "finally ---> final\n",
      "finalize ---> final\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemming = PorterStemmer()  # Initializing PorterStemmer\n",
    "\n",
    "for word in words:\n",
    "    print(word+\" ---> \"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7112912a-167c-487a-9252-be70c1097449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is some major issues with this approach, LIKE\n",
    "\n",
    "stemming.stem(\"congratulations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2386ce08-489f-4d1a-82ad-6d879fdd567e",
   "metadata": {},
   "source": [
    "### RegexpStemmer class\n",
    "NLTK has RegexpStemmer class with the help of which we can easily implement regular expression stemmer algorithm. It basically takes a single regular expression and removes \n",
    "any prefix or suffix that matches the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5629f9c1-3041-434f-b0ae-b6bc68e53f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n",
      "ingeating   <--- Look at it\n",
      "edibl\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$|ed$', min=4)\n",
    "\n",
    "print(reg_stemmer.stem(\"eating\"))\n",
    "print(reg_stemmer.stem(\"eats\"))\n",
    "print(reg_stemmer.stem(\"ingeating\" + \"   <--- Look at it\"))\n",
    "print(reg_stemmer.stem(\"edible\"))\n",
    "print(reg_stemmer.stem(\"history\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e9f18-58cd-47b6-a6e5-9126a77f1fe4",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e7de941-f59b-46a1-ab13-88450878a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowball :   eating ---> eat\n",
      "RegexpStemmer :   eating ---> eat\n",
      "PorterStmmer : eating ---> eat\n",
      "\n",
      "Snowball :   eats ---> eat\n",
      "RegexpStemmer :   eats ---> eat\n",
      "PorterStmmer : eats ---> eat\n",
      "\n",
      "Snowball :   eaten ---> eaten\n",
      "RegexpStemmer :   eaten ---> eaten\n",
      "PorterStmmer : eaten ---> eaten\n",
      "\n",
      "Snowball :   writing ---> write\n",
      "RegexpStemmer :   writing ---> writ\n",
      "PorterStmmer : writing ---> write\n",
      "\n",
      "Snowball :   writes ---> write\n",
      "RegexpStemmer :   writes ---> write\n",
      "PorterStmmer : writes ---> write\n",
      "\n",
      "Snowball :   written ---> written\n",
      "RegexpStemmer :   written ---> written\n",
      "PorterStmmer : written ---> written\n",
      "\n",
      "Snowball :   programming ---> program\n",
      "RegexpStemmer :   programming ---> programm\n",
      "PorterStmmer : programming ---> program\n",
      "\n",
      "Snowball :   programs ---> program\n",
      "RegexpStemmer :   programs ---> program\n",
      "PorterStmmer : programs ---> program\n",
      "\n",
      "Snowball :   programmed ---> program\n",
      "RegexpStemmer :   programmed ---> programm\n",
      "PorterStmmer : programmed ---> program\n",
      "\n",
      "Snowball :   history ---> histori\n",
      "RegexpStemmer :   history ---> history\n",
      "PorterStmmer : history ---> histori\n",
      "\n",
      "Snowball :   finally ---> final\n",
      "RegexpStemmer :   finally ---> finally\n",
      "PorterStmmer : finally ---> final\n",
      "\n",
      "Snowball :   finalize ---> final\n",
      "RegexpStemmer :   finalize ---> finaliz\n",
      "PorterStmmer : finalize ---> final\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for word in words:\n",
    "    print(\"Snowball :   \"+word+\" ---> \"+snowball_stemmer.stem(word))\n",
    "    print(\"RegexpStemmer :   \"+word+\" ---> \"+reg_stemmer.stem(word))\n",
    "    print(\"PorterStmmer : \"+word + \" ---> \"+stemming.stem(word)+\"\\n\") ## for comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dae18edf-e594-4e18-b7e9-6b9a50e8842b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli', 'fair', 'sport')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly'), stemming.stem('sportingly'),snowball_stemmer.stem('fairly') , snowball_stemmer.stem('sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab32c99-a1b7-421c-9c50-273c8ee4669d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
